<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>internet_behavior_project</artifactId>
        <groupId>com.anryg.bigdata</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>flink-coding</artifactId>

    <name>flink-coding</name>
    <!-- FIXME change it to the project's website -->
    <url>http://www.example.com</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>

        <flink.version>1.15.2</flink.version>
        <hadoop.version>3.1.1</hadoop.version>
    </properties>

    <dependencies>
        <!--DataStream API需要-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_2.12</artifactId><!--如果是用scala语言，需要指定scala版本-->
            <version>${flink.version}</version>
            <exclusions>
                <exclusion>
                    <artifactId>commons-math3</artifactId>
                    <groupId>org.apache.commons</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId><!--不用指定scala版本-->
            <version>${flink.version}</version>
        </dependency>
        <!--读取kafka需要-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka</artifactId><!--不用指定scala版本-->
            <version>${flink.version}</version>
        </dependency>
        <!--DataStream转Table-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-api-scala-bridge_2.12</artifactId><!--需要指定scala版本，这个需要跟flink-streaming-scala_2.12保持一致，如果上面是java，这里就对应的java-->
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-table-planner_2.12</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <!--提供ES连接-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-elasticsearch7</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <!--添加Hadoop依赖，数据写hdfs需要-->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
            <exclusions>
                <exclusion>
                    <artifactId>commons-compress</artifactId>
                    <groupId>org.apache.commons</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
            <exclusions>
                <exclusion>
                    <artifactId>commons-compress</artifactId>
                    <groupId>org.apache.commons</groupId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <!--用于识别CSV文件-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-csv</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-hadoop-compatibility_2.12</artifactId>
            <version>${flink.version}</version>
        </dependency>
        <!--json转换-->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.71</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <sourceDirectory>src/main/scala</sourceDirectory> <!--指定打包的位置，默认只打src/main/java目录，且只能打包一个目录-->
        <testSourceDirectory>src/main/test</testSourceDirectory>
        <!--<finalName>kafka-sparkstreaming</finalName>--> <!--第一个打的源码jar包的名字，可以不用-->
        <plugins>
            <!--该插件可以用来解决项目jar包（类）与server端冲突问题-->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.0</version><!--20220418因spark2升级到spark3升级到用3.2.0-->
                <configuration>
                    <shadedArtifactAttached>true</shadedArtifactAttached>
                    <shadedClassifierName>with-dependencies</shadedClassifierName><!--给依赖包添加后缀名-->
                    <artifactSet>
                        <includes>
                            <include>*:*</include>
                        </includes>
                        <excludes>
                            <exclude>junit:junit</exclude> <!--排除jar包-->
                        </excludes>
                    </artifactSet>

                    <filters><!--去掉META-INF文件中可能出现的非法签名文件-->
                        <filter>
                            <artifact>*:*</artifact>
                            <excludes>
                                <exclude>META-INF/*.SF</exclude>
                                <exclude>META-INF/*.DSA</exclude>
                                <exclude>META-INF/*.RSA</exclude>
                            </excludes>
                        </filter>
                    </filters>
                    <minimizeJar>false</minimizeJar> <!--使jar包最小化，有时候会将一些间接依赖的类打掉，慎用-->
                    <!--<encoding>UTF-8</encoding>-->
                    <!--<appendAssemblyId>true</appendAssemblyId>
                    <descriptors>
                        <descriptor>package.xml</descriptor> &lt;!&ndash;这个用不上了&ndash;&gt;
                    </descriptors>
                    <createDependencyReducedPom>false</createDependencyReducedPom>-->
                </configuration>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <!--<relocations>
                                <relocation> &lt;!&ndash;将会冲突的类重命名&ndash;&gt;
                                    <pattern>com.google.guava</pattern>
                                    <shadedPattern>com.shade2.google.guava</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>com.google.common</pattern>
                                    <shadedPattern>com.shade2.google.common</shadedPattern>
                                </relocation>
                                <relocation>
                                    <pattern>com.google.thirdparty</pattern>
                                    <shadedPattern>com.shade2.google.thirdparty</shadedPattern>
                                </relocation>
                            </relocations>-->
                            <transformers><!--20190829添加，解决An SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Lucene50' does not exist.  You need to add the corresponding JAR file supporting this SPI to your classpath的问题-->
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer" />
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer" />
                                <!--<mainClass></mainClass>--> <!--主类名-->
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <!--增加可以打包多个source的插件-->
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>build-helper-maven-plugin</artifactId>
                <version>3.0.0</version>
                <executions>
                    <execution>
                        <id>add-source</id>
                        <phase>generate-sources</phase>
                        <goals>
                            <goal>add-source</goal>
                        </goals>
                        <configuration>
                            <sources>
                                <source>src/main/java</source>  <!--增加打包的目录-->
                            </sources>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <!--之前的，只能指定一个资源进行打包,有时候不需要，但有时候又必须要，否则scala代码无法打包-->
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.1</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
